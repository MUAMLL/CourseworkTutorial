{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f355977e-3782-4333-9b4f-f62afbc39d1c",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "This lab will introduce Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb62f5b-b39a-4986-b23f-e4beab1b93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00882d-de0e-4439-b818-e336591e4849",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's begin by preparing a dataset for this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae664716-8483-44f6-bb56-23de443b34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(\"../datasets/winequality-red.csv\", sep=\";\")\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.iloc[:, :-1].values, df.iloc[:, -1], test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e1161-a403-4b6c-9b07-8d794977ba78",
   "metadata": {},
   "source": [
    "## What is a Support Vector Machine? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32b9b6-8101-413b-ad02-dd20f6b5a7fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Another classification algorithm that utilizes gradient descent for optimization is the Support Vector Machine (SVM).\n",
    "\n",
    "As with the perceptron and the Perceptron, the SVM is a linear classifier, attempting to find a hyper-plane that separates the classes in feature space (i.e., the dimension in which the features exist).\n",
    "\n",
    "Unlike our Perceptron, however, the SVM attempts to _maximize_ the separation of the classes. It does this by optimizing the amount of space between the classes in feature space:\n",
    "\n",
    "<img src=\"../images/support_vectors.jpeg\" width=600 align=\"left\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be4817-c229-4a2f-aa3d-51c3dfc250fd",
   "metadata": {},
   "source": [
    "<span style=\"font-size:10px\"><a href=\"https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\">Image Source</a></span>\n",
    "\n",
    "Also similar to an Perceptrons, SVM's are optimized using gradient descent to minimize a cost function, but in an SVM that cost function is built to maximize the distance between classes rather than only classify each point correctly as in a Perceptron.\n",
    "\n",
    "As you can see in the figure above, the cost function is attempting to measure the space between the classes in feature space using support vectors.\n",
    "\n",
    "## The Kernel Trick\n",
    "\n",
    "<img src=\"../images/kernel_trick.png\" width=700 align=\"left\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43baef5e-05df-4476-8647-94b74128efe7",
   "metadata": {},
   "source": [
    "<span style=\"font-size:10px\"><a href=\"https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f\">Image Source</a></span>\n",
    "\n",
    "The SVM, like the perceptron, is attempting to find a hyperplane that can separate the classes in our data. Without any improvement beyond what we have discussed, the SVM would be restricted, like the perceptron, to only being applicable to linearly separable problems.\n",
    "\n",
    "However, recall how linear models, utilize a non-linear function, $f$? This non-linearity was in essence mapping our features into a higher dimensional space, where previously inseprable features now become linearly separable.\n",
    "\n",
    "We can follow this same process in our SVM. First, we can apply a transformation to our data to map it to a linearly seperable feature space. We can then operate in this higher dimensional space where the features are linearly separable to determine our decision boundaries between classes, and then map back down to the original feature space.\n",
    "\n",
    "The problem is that if we have many data points, and we map all of the data points' features into higher dimensions and then opearate, the memory and processing requirements will scale to unrealistic levels. As it turns out, we don't need to actually apply this transformation to a higher dimension, we can just store pairwise similarities between the two feature spaces (the original and the higher dimension one), and this representation is sufficient to find decision boundaries. This process is called the **kernel trick**.\n",
    "\n",
    "## Multi-class Classification with SVM\n",
    "\n",
    "While the kernel trick allows the SVM to work on problems that are not linearly separable, it is still restricted to only functioning with 2-classes in its naive form.\n",
    "\n",
    "Because SVMs are attempting to maximize the distance between two classes by maximizing the length of support vectors, it can only take on 2-classes at a time.\n",
    "\n",
    "We can accomplish multi-class classification with SVMs using a _over versus the rest_, or OVR, scheme. In this scheme, we train _n_classes_ SVMs, each one trained as a 2-class SVM, with the postive class being one of the classes in our dataset and the the negative class being all other classes.\n",
    "\n",
    "So in a three class problem with classes A, B, C, the OVR scheme is 3 SVMs:\n",
    "\n",
    "- class A vs classes B & C\n",
    "- class B vs classes A & C\n",
    "- Class C vs classes B & C\n",
    "\n",
    "This means that for every class we add, we are adding another model to be trained and tested. Thankfully, `sklearn` abstracts this process for us, and we need to only give it our data and tell it to use a `ovr` scheme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2899ea-b6e7-4cbd-8abe-b1c691659868",
   "metadata": {},
   "source": [
    "## Using an SVM\n",
    "\n",
    "Now that we understand what a SVM is, how it is trained, and how it solves linearly inseparable problems, let's build one. The name of the function in `sklearn` is the Support Vector Classifier, or `SVC`. We'll choose to parameterize our SVM with a polynomial kernel of degree 3 (a cubic function) and a maximum number of iterations of 50:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b32f174-a9f2-4192-a7f7-85864efa17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(\n",
    "    kernel=\"poly\", degree=3, verbose=False, max_iter=2500, decision_function_shape=\"ovr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846db6aa-497c-457a-8b22-442ccb7aefe4",
   "metadata": {},
   "source": [
    "Now let's train it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5d990-2730-4c82-b3b2-d8adf53b8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234375d6-d0c8-480d-a765-2dfdd9b80868",
   "metadata": {},
   "source": [
    "Now, let's score it against the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfed41-acc3-437d-8179-335dbc56df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af9a2b-3255-42ee-b520-96c6e1587147",
   "metadata": {},
   "source": [
    "SVMs are very sensitive, as are all Machine Learning models, to their hyperparameters, let's look at how the choice of Kernel and Regularization affects performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821fbb3-4542-4daf-a520-7061826b05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"rbf\", C=10)\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c107dc-737e-4a7c-8981-1550f6b0d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"poly\", degree=5, C=10)\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abea845-f0d1-49cc-b586-d4e420a5fd02",
   "metadata": {},
   "source": [
    "## <span style=\"background:yellow\">Your Turn</span>\n",
    "\n",
    "Train and test a SVC model with degree of 2 and poly kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "843da06c-495d-42c2-8cbc-66deadec9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- Your Code Here -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
