{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier Lab\n",
    "This lab will introduce you to the K-Nearest Neighbor Classifier. Let's import our modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2 as cv\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import (\n",
    "    local_binary_pattern as lbp,\n",
    "    hog\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import metrics\n",
    "from torchvision.datasets import MNIST\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that we will use for this lab is the MNIST<sup><a href=\"https://en.wikipedia.org/wiki/MNIST_database\">1</a></sup> dataset. This dataset is a handwritten digit dataset, with 10 classes for the 10 digits: 0,1,2..9.\n",
    "\n",
    "\n",
    "We can load the dataset using the `torchvision` package. We will discuss this package in far more detail in later portions of this module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=Path.home() / \"data\" / \"MNIST\", download=True, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is loaded as a set of image and label pairs. The images are loaded as pillow (`PIL`) images, but we can convert them to the `np.ndarray` we are used to by using the `np.asarray` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = dataset[0]\n",
    "\n",
    "img = np.asarray(image)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Features: Local Binary Patterns\n",
    "\n",
    "We are now going to attempt to classify the MNIST dataset, but we first will need to extract _features_ from the imagery. To generate features, we will use [local binary patterns (LBP)](https://scikit-image.org/docs/0.24.x/auto_examples/features_detection/plot_local_binary_pattern.html).\n",
    "\n",
    "The LBP feature descriptor functions by dividing an image into windows, comparing those windows to its neighbors, and assigning a binary 0 where the center pixel's value is greater than the neighbors, and 1 otherwise.\n",
    "\n",
    "This binary image then undergoes histogram normalization, and the resulting windows are then concatenated to give a feature descriptor of the entire image. \n",
    "\n",
    "Let's generate the LBP output for our sample image using the `scikit-image` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 2\n",
    "n_points = 8 * radius\n",
    "image_features = lbp(np.asarray(image), n_points, radius)\n",
    "\n",
    "plt.imshow(image_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the LBP features for all the images in our training dataset. The classifier that we are going to use, the K-Nearest Neighbor (KNN) classifier, requires single dimensional features, so we will have to flatten the feature vector to 1-d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize two numpy tensors\n",
    "features = np.empty((len(dataset), img.size))\n",
    "labels = np.empty(len(dataset), dtype=int)\n",
    "# for each image and label pair\n",
    "for i, (pil_image, label) in enumerate(dataset):\n",
    "    # convert from PIL image to tensor\n",
    "    img_tensor = np.asarray(pil_image)\n",
    "    # generate the features using same parameters as our sample above\n",
    "    # and flatten the 2-d output to 1-d\n",
    "    features[i] = lbp(img_tensor, n_points, radius).flatten()\n",
    "    # save the label to our labels array\n",
    "    labels[i] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The KNN Classifier\n",
    "Now that we have generated features using the local binary pattern, we need to introduce our classifier. The first classifier that we are going to introduce is the K-Nearest Neighbor (KNN) classifier. This classifier functions by, for each sample, looking the the $k$ closest samples in feature space, and assigning the class most common among those $k$ neighbors.\n",
    "\n",
    "Let's consider a 2-class problem in $\\mathbb{R}^2$:\n",
    "$$\n",
    "X = \\{ [-1, 1], [-1, 2], [3,4], [2,4] \\}\n",
    "$$\n",
    "$$\n",
    "Y =\\{ 0, 0, 1, 1 \\}\n",
    "$$\n",
    "\n",
    "Let's view this problem on a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-1, 1], [-1, 2], [3, 4], [2, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1], c=[\"r\", \"r\", \"b\", \"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say a new point (1, 3) is added. What class does it belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 0], x[:, 1], c=[\"r\", \"r\", \"b\", \"b\"])\n",
    "plt.scatter(1, 3, c=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbor classifier would solve this problem by finding the $k$ nearest points. For our toy problem, let's use $k$ of 3. So what are the 3 closest points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = np.array([1, 3])\n",
    "distances = [np.linalg.norm(pt - _x) for _x in x]\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the three closest points are $[-1, 2], [3,4], [2,4]$. Now that we have found the three closest neighbors, we classify the point by assigning the class that **most** of its neighbors belong to, which in this case is class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying KNN to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With features now generated and our understanding of the KNN classifier, we can initialize and train our classifier. We are going to initialize our classifier with default parameters, which as described in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), means that we will use an $L_2$, or euclidean, distance metric and a $k$ of 5.\n",
    "\n",
    "The name of the function to train the KNN classifier is the `fit` function, which takes in 2 parameters:\n",
    "1. features - a NxM vector with N samples and M features\n",
    "2. Labels - a 1-d vector of length N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a KNN classifier with default parameters\n",
    "knn = KNN()\n",
    "# train (fit) our classifier on our generated features and labels\n",
    "knn.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier has now been trained. Let's examine how our classifier performed by checking its training accuracy. The training accuracy is the accuracy of the model on the same set of data upon which the model is trained.\n",
    "\n",
    "However, our training dataset is 60,000 images, so let's choose a random subset of 5000 images to get our training accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 5000 random integers between 0 and 4999\n",
    "sample_idx = random.sample(range(len(dataset)), 5000)\n",
    "# get the features at those indices\n",
    "sample_features = [features[i] for i in sample_idx]\n",
    "# get the labels at those indicies\n",
    "sample_labels = [labels[i] for i in sample_idx]\n",
    "\n",
    "# use knn.predict to generate predicted class for our sample features\n",
    "o = knn.predict(sample_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our predicted and target labels now generated, we can calculate our accuracy using the `sklearn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(sample_labels, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our classifier has achieved excellent training accuracy. Let's load the test portion of the dataset so we can test our model on data is has not yet seen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root=Path.home() / \"data\" / \"MNIST\", download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again need to generate the LBP features and labels from the test imagery, following the same set of steps as the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.empty((len(test_dataset), img.size))\n",
    "test_labels = np.empty(len(test_dataset), dtype=int)\n",
    "for i, (pil_image, label) in enumerate(test_dataset):\n",
    "    test_features[i] = lbp(np.asarray(pil_image), n_points, radius).flatten()\n",
    "    test_labels[i] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now again use the `predict` function with our features vectors to generate predicted classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = knn.predict(test_features)\n",
    "metrics.accuracy_score(test_labels, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows the distribution of predictions between the predicted and true labels. Along the diagonal of the matrix is where the true and predicted labels match (TP), and off the diagonal indicates places where the true and predicted labels disagree (FP and FN). We can generate this matrix easily with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_labels, o)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Other Feature Extractors\n",
    "Thus far we have only used the LBP feature extractor to generate features for our KNN classifier. However, feature descriptors for KNN image classification can be generated by many methods beyond just the local binary pattern.\n",
    "\n",
    "Let's now use the [Histogram of Oriented Gradients (HOG)](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html) to generate feature descriptors for our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIENTATIONS = 8\n",
    "CELLS_PER_BLOCK = 3\n",
    "\n",
    "hog_features_train = np.empty((len(dataset), ORIENTATIONS * CELLS_PER_BLOCK**2))\n",
    "\n",
    "for i, (pil_image, label) in enumerate(dataset):\n",
    "    hog_features_train[i] = hog(\n",
    "        np.asarray(pil_image),\n",
    "        orientations=ORIENTATIONS,\n",
    "        cells_per_block=(CELLS_PER_BLOCK, CELLS_PER_BLOCK),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_features_test = np.empty((len(test_dataset), ORIENTATIONS * CELLS_PER_BLOCK**2))\n",
    "\n",
    "for i, (pil_image, label) in enumerate(test_dataset):\n",
    "    hog_features_test[i] = hog(\n",
    "        np.asarray(pil_image),\n",
    "        orientations=ORIENTATIONS,\n",
    "        cells_per_block=(CELLS_PER_BLOCK, CELLS_PER_BLOCK),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a new KNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN()\n",
    "knn.fit(hog_features_train, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our trained model to predict on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(hog_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the accuracy of our model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our HOG based KNN classifier performed very well, but not quite as well as our LBP based classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## <span style=\"background:yellow;\">**Your Turn**</span>\n",
    "\n",
    "Train a new KNN classifier with the number of neighbors set to 9, and print its test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your Notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
