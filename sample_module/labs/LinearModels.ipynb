{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f355977e-3782-4333-9b4f-f62afbc39d1c",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "This lab will introduce Linear Models for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb62f5b-b39a-4986-b23f-e4beab1b93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00882d-de0e-4439-b818-e336591e4849",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Let's begin by preparing a dataset for this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae664716-8483-44f6-bb56-23de443b34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(\"../datasets/winequality-red.csv\", sep=\";\")\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.iloc[:, :-1].values, df.iloc[:, -1], test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e1161-a403-4b6c-9b07-8d794977ba78",
   "metadata": {},
   "source": [
    "## What is a linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb937a9-6827-4c48-96a0-141d655800ed",
   "metadata": {},
   "source": [
    "A linear classifier is a machine learning model that classifies samples using a linear combination of their features. \n",
    "\n",
    "Formally, for a sample, $x$ with feature vector, $\\vec{x}$, the output of a linear model is a linear combination of those features with corresponding weights, $\\vec{w}$:\n",
    "\n",
    "$$\n",
    "y = f(\\vec{w}\\cdot\\vec{x}) = f \\left( \\sum_j w_j x_j \\right)\n",
    "$$\n",
    "\n",
    "To train a linear model, we need to find the set of weights, $\\vec{w}$, that minimizes classification errors. That is what the `fit()` function is doing in SKLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9d7f1-6cb6-41c8-afeb-5bb1a26e89e6",
   "metadata": {},
   "source": [
    "## Building a Train and Eval Function\n",
    "\n",
    "Let's build a function that will train and test a model on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aeba72-09d5-49cb-a8e1-a2b6f7922076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0a14e-6248-4950-a511-45452daeedb9",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The first linear model we'll use is Logistic Regression. Logistic regression is a linear model that uses a logistic equation as the function $f$ in the above equation:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "Let's use our train_and_test function to train a Logistic Regression Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb54897-d86f-419b-9247-2ffcf0bfb654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=500)\n",
    "train_and_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c7ce4-7436-4018-89f9-515b8360a508",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "<img src=\"../images/perceptron.png\" width=500 />\n",
    "\n",
    "The perceptron is a linear model that utilizes a step function as its activation function $f$:\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "0 & x \\leq 0 \\\\\n",
    "1 & x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Perceptron Learning Algorithm\n",
    "The perceptron optimization algorithm functions by optimizing a cost function. In particular, it attempts to minimize the amount of error (or cost) that is generated by classifying the train dataset. \n",
    "\n",
    "It does this by measuring the error of the classifier (i.e., a misclassification) and then updating its own weights proportional to that error. The key idea is that by iterating though the training data enough times, the perceptron will learn how to separate the different classes present in the data. \n",
    "\n",
    "The perceptron is also our first introduction to neural networks, a family of machine learning algorithms modeled after the neurons in the brain. The perceptron training algorithm, described below, is a foundational algorithm for understanding how neural networks learn.\n",
    "\n",
    "The training algorithm for a perceptron is a loop. For each sample during training:\n",
    "1. Aggregation - Take the dot product of the incoming features, $x$, and the weights, $w$, and add a bias, $b$: \n",
    "$$\n",
    "v = w \\cdot x + b= \\sum_i w_ix_i + b\n",
    "$$\n",
    "2. Activation - Activation determines the output of the perceptron. If the aggregated local induced field ($v$) is greater than or equal to 0, we assign a value of 1, but if the $v$ is less than 0 we assign a 0.\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 & v \\geq 0 \\\\\n",
    "0 & v < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "3. Error calculation - Determine the classification error using a cost function, $C$, which takes in the output of the perceptron, $y$, and the desired label, $d$, and returns a numerical cost:\n",
    "$$\n",
    "e = C(y, d) = y - d\n",
    "$$\n",
    "4. Weight Update - Update the weights and bias using the error and a scaling factor, known as a learning rate, $\\alpha$. The learning rate allows us to decide how large of an update to apply to the weights\n",
    "$$ \n",
    "w_{new} = w_{old} - \\alpha ex\n",
    "$$\n",
    "\n",
    "Let's now see the perceptron in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde0c4f-2ba9-4e0e-9a2b-3072b09571ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron(verbose=0, max_iter=5000)\n",
    "train_and_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5904d-7820-4343-9a84-65d4cb333ef5",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) Classifier\n",
    "\n",
    "The perceptron classifier described above is a special case of a SGD Algorithm, which itself is a linear algorithm. The perceptron learning algorithm is also known as Gradient Descent, and with some minor modifications, is known as Stochastic Gradient Descent (SGD). \n",
    "\n",
    "The SGD Classifier differs from the Perceptron in that we can change how the loss (Step 3) is calculated, while for the Perceptron, we can only use the `perceptron` loss function. \n",
    "\n",
    "Let's use the SGD Classifier on our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff79965-62ba-4035-bb0a-57c75483a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(max_iter=5000, loss=\"squared_hinge\")\n",
    "train_and_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abea845-f0d1-49cc-b586-d4e420a5fd02",
   "metadata": {},
   "source": [
    "## <span style=\"background:yellow\">Your Turn</span>\n",
    "\n",
    "Train and test an SGD Classifier with Huber loss with max iterations of 2500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843da06c-495d-42c2-8cbc-66deadec9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- Your Code Here -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
