{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94876842-7969-4d43-8d54-f001143fffce",
   "metadata": {},
   "source": [
    "# Module 2 Practice: Linear Models, SVM, and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627014b7-cb9c-496e-891f-30838a76842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "DATASET = \"../datasets/titanic.csv\"\n",
    "assert os.path.exists(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096763c-f26d-4a08-8822-2cbb49738c54",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "In order to practice using KNN, Linear Models, or SVMs, we'll need to have a train and test dataset. Let's use the titanic dataset that was imported above. In the cell below, load the data into a DataFrame and separate the features into `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722ff60-04bd-4bf8-9c89-9e50dd9f5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb444d2e-b210-49e1-ba40-a9daa12ee899",
   "metadata": {},
   "source": [
    "## Building a Train and Test Function\n",
    "\n",
    "In order to best evaluation our models, let's build a function that will take in the features and labels to use for evaluation, splits the data into train test based on a passed-in variable, and evaluates a model that was also passed in. Be sure to set the random state so that each train/test split is identical between runs. Return the accuracy of the model on the test partition from the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e62984-c16a-4563-9694-49701acb788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----\n",
    "# Your code here\n",
    "\n",
    "\n",
    "def titanic_model_eval(model, X, y, test_size):\n",
    "    # split the data\n",
    "\n",
    "    # fit the model\n",
    "\n",
    "    # predict with the model\n",
    "\n",
    "    # calculate accuracy\n",
    "\n",
    "    # return the accuracy\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665dc8f-bfc7-4408-8f98-7eff54e2a69a",
   "metadata": {},
   "source": [
    "## Training Linear Models\n",
    "\n",
    "Let's test a Logistic Regression model. Create a linear regression model with an $L_1$ penalty, a `liblinear` solver, and a regularization constant of 1.5. Allow a maximum number of iterations to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae671c90-97ff-4957-8282-bdd22826160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838451ba-a12f-4d7b-82fb-2f4d5c751c29",
   "metadata": {},
   "source": [
    "Now, use the function we built above to get the test accuracy on the titanic dataset with an 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c157ce-d747-4bcd-9dd8-00ffd0e1b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927acaf8-561b-4d30-aab2-d47ed3254981",
   "metadata": {},
   "source": [
    "We can also use our function to see how our performance changes as we adjust the train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fa363-4425-4d88-ba94-5600b5e48b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_test = np.linspace(0, 1, 11)[1:-1]\n",
    "scores = [titanic_model_eval(model, X, y, P) for P in pct_test]\n",
    "\n",
    "plt.bar(pct_test, scores, width=0.08)\n",
    "plt.xticks(pct_test)\n",
    "plt.ylim(min(scores) - 0.05, max(scores) + 0.02)\n",
    "plt.xlabel(\"Percent Test\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.gcf().set_size_inches(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfea055-6c36-4da7-91ba-6bfdd55bebae",
   "metadata": {},
   "source": [
    "## MLP and Max Iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff430d-065b-44fc-83ef-0e7720982650",
   "metadata": {},
   "source": [
    "Train and test a Perceptron model with $L_2$ penalty, an alpha of 0.005, and a max iteration of 50. Use a train/test split of 70/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a21000-9ece-43cc-9e54-830ee7abb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d8d19-4be1-46d0-a921-f81e06e5f85b",
   "metadata": {},
   "source": [
    "We've seen a Perceptron classifier, but there is another Gradient Descent based classifer that stacks multiple Perceptrons together into layers, known ans a [Multi-Layer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), or MLP. \n",
    "\n",
    "Let's train an MLP on the data and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258231e8-fd84-4fea-9921-595e05e7e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_model_eval(MLPClassifier(solver=\"sgd\", max_iter=50), X, y, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abec9f-08c9-4f76-a278-f687117e68fd",
   "metadata": {},
   "source": [
    "So, it performs better than the Perceptron! Interesting! Let's try varying the maximum number of iterations and seeing how that affects the performance. Try at least 5 different max iteration values and plot them in a bar plot, like we did for the Logistic Regression. Be sure to use an SGD solver, and set the random state of the classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7e293-c2eb-4e92-9113-5f7c2c281cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488b81e-c88d-439f-8dde-27d2dd4e9063",
   "metadata": {},
   "source": [
    "What you should have seen is that the MLP Classifier is very sensitive to the maximum number of iterations used in training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dea656-18f9-4fd9-bd4f-d9350a32d722",
   "metadata": {},
   "source": [
    "## SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4f45c-bca3-4177-b4e9-f1ecfd2f76d3",
   "metadata": {},
   "source": [
    "Finally, let's build a couple of SVM models on the Titanic Data. \n",
    "\n",
    "In the cell below, train an SVM Classifier with default parameters on the titanic data, using a train test split of 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda4a07-3560-4c99-881a-43504d2d1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72582657-549d-49d0-a651-5a9686b1cc76",
   "metadata": {},
   "source": [
    "SVMs are most sensitive to their kernel and their regularization. Train at least 2 more SVM models, varying these parameters are you see fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8713437-294f-4ee7-891d-0b6652263191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2ab02-07a3-4434-b215-ca37dac358db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49f611-e25f-49fb-b74f-cefd12f9850b",
   "metadata": {},
   "source": [
    "## Comparing Models\n",
    "\n",
    "A common practice in Machine Learning is comparing the performance of models. \n",
    "\n",
    "In the cell below, split the data into train and test with a 90/10 split. Then create one of each of the models we used above: Logistic Regression, Perceptron, MLP, and SVM. \n",
    "\n",
    "Train and test each one on your train/test split data. Use the same max iteration for all of them, but you are free to select other parameters as you see fit. Then, rank the models by their accuracy.\n",
    "\n",
    "Do not use the `titanic_model_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1cf5c-31e5-4743-bab2-11ba99a64404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
