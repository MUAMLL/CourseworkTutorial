{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Exercise\n",
    "This exercise will ask you to train various classifiers on the MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2 as cv\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import (\n",
    "    local_binary_pattern as lbp,\n",
    "    hog\n",
    ")\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import metrics\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm as progressbar\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset\n",
    "We are again going to use the MNIST Dataset, which is a handwritten digit recognition dataset, which we can formulate as a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root=Path.home() / \"data\" / \"MNIST\", download=True, train=True)\n",
    "test_dataset = MNIST(root=Path.home() / \"data\" / \"MNIST\", download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## #1 Data Preparation\n",
    "\n",
    "In the cell below, extract features from MNIST using either Local Binary Pattern or Histogram of Oriented Gradients, and then split the train dataset into training and validation (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2 Running Nearest Neighbors\n",
    "Train and test a Nearest Neighbor classifier with 7 of the nearest neighbors using the features from #1. Print the validation and  test accuracy to the screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 Running a Perceptron\n",
    "Train, validate, and test a Perceptron on the MNIST dataset. Use a learning rate of 0.025 and a max iteration of 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4 Running an MLP\n",
    "Train, validate, and test a [**Multi-Layer Perceptron (MLP)**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) on the MNIST dataset. Use an Adam solver, a batch size of 128, a max iter of 100, and a learning rate of 0.025. An MLP is a linear model, which function as sets of Perceptrons organized into layers and are used extensively in Deep Neural Networks, which we will see in a later module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5 Running Logistic Regression\n",
    "Train, validate, and test a Logistic Regression Classifier on the MNIST dataset. Use an $L_1$ penalty, regularization constant of 2, and a maximum number of iterations of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #6 Running an SVM\n",
    "Train and test a [Linear SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) on the MNIST dataset. Use a max iteration count of 100 with hinge loss and set the algorithm to solve the dual optimization problem. Print the validation and test accuracy score to the screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #7 Reflection & Conclusion\n",
    "\n",
    "What are you conclusions about all of these models? Feel free to discuss things such as:\n",
    "- How linear models compare against each other, both in terms of accuracy and time needed to train?\n",
    "- Comparisons between SVMs and Linear Models\n",
    "- Insights into Model Performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#--------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your Notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
